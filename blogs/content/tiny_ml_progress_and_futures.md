Currently as of the time of writing this, I am a SWE at Google working on building embedded, low-latency Machine Learning algorithms for the Pixel Watch series. The reason why this is the first blog (and largely why I wanted to write blogs in general) is because I wanted to gain some contextual/deeper insight into the problem spaces that I work on. This blog isn’t meant to be a source of truth or knowledge base, so I apologize if I say things incorrectly. Everything I write here are just my own inferences (*ha*) and thoughts derived from reading this whitepaper. On with the review\!

A lot of the content in this paper rings true to me in many faucets, and the importance of TinyML to me as a person that works on it in my day-to-day, truly cannot be overstated. There was a stat from Harvard in 2018 that said that we do not use 99% of all data collected from edge devices. That’s crazy, right? We have so much data floating around, and Big Data especially seems to really care about collecting massive suaves of it \-- but we don’t utilize the collected data nearly as much on these devices (trillions of them by the way) as much as we’d have hoped. 

TinyML, unlike traditional DL models, comes at a very low compute cost. Resource constrained environments (e.g an MCU) really need to be built with power consumption, compute cost and memory in mind. For example, MCUs have SRAM and they’re only about 256kB. The table from the paper says it nicely \-- “Microcontrollers have 3 orders of magnitude less memory and storage compared to mobile phones, and 5-6 orders of magnitude less than cloud GPUs” (Lin et al, 2). A part of this from what I’m reading seems to be because of the focus of what an optimization problem might look like for an efficient DP ML model. Things like latency reduction/FLOPs/parameters get optimized but peak memory usage gets swept under the rug a bit as less-constrained environments typically do not need to worry about the peak memory usage (although it does look like the size has gotten smaller over the years).

TO BE CONTINUED...